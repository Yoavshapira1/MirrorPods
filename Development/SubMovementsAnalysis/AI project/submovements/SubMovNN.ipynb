{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjCW_g6Bodi8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset, Dataset, DataLoader, RandomSampler\n",
    "from torch import Tensor, optim\n",
    "torch.manual_seed(0)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "import numpy as np\n",
    "root_dir = ''\n",
    "!pip install tsai\n",
    "from tsai.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDIQZpa0pIyo",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **Parameters**\n",
    "Dataset generation parameters,\n",
    "\n",
    "Model trining hyper-parameters,\n",
    "\n",
    "and General workflow parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1WcTYFAwaoE",
    "tags": []
   },
   "source": [
    "### SM generation\n",
    "\n",
    "This section defines the par|ameters used to generate the data set.\n",
    "\n",
    "The different conditions suppose to help imitating a human movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "9QBFhYGKpIdw"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data_with_stops = 0.001       # chances for a profile to include stop in the movement\n",
    "overlapping_range = (0.25, 0.75)    # if not include stop, this is the overlapping range\n",
    "with_stops_range = (1., 1.15)       # if does include stops, this is the stop length\n",
    "t_first_range = 1.                  # last time (in sec.) for the first sub movement to start\n",
    "duration_min = 0.167                # minimal duration\n",
    "duration_max = 1.                   # maximal duration\n",
    "amp_min = 0.1                       # minimal amplitude - not including neglect movements in the data\n",
    "amp_max = 1                         # maximal amplitude - reflects the maximal displacement for positive direction\n",
    "amp_duration_ratio = 2              # limits the amplitude peaks accordingly to the movements duration\n",
    "amount_per_seconds = {\"amount\": 4,  # maximal sub movements per seconds limitation\n",
    "                      \"seconds\": 1.8}\n",
    "\n",
    "s_rate = 100                        # sample rate\n",
    "len_synthesize = 5                  # length of the first synthesized vector in seconds\n",
    "real_len = 4                        # length of the actual vector after trimming the end (allowing trimmed SMs)\n",
    "max_M = 7                           # maximal amount of submovements\n",
    "dim = 1                             # The spatial dimension\n",
    "\n",
    "# NOTICE: the shape of the\n",
    "# parameters matrix for every velocity\n",
    "# vector is always: (max_M, 2 +dim)\n",
    "\n",
    "filter_size_mean = 4                # the higher the value, the subtler filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbSqY_RPwdBJ",
    "tags": []
   },
   "source": [
    "### Model Training\n",
    "\n",
    "This section defines the default values for the hyper-parameters. Most of them are not in use explicitly, but just as a default values for the different pytorch models I have implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "hqgr9x8uwhc6"
   },
   "outputs": [],
   "source": [
    "# Regularizations strenghts\n",
    "sgl_lambda = 0.\n",
    "l2_reg = 0.0001\n",
    "\n",
    "# learning rate\n",
    "lr = 0.\n",
    "\n",
    "# LSTM default settings\n",
    "hidden_size = 64\n",
    "num_layers = 5\n",
    "\n",
    "# relevant for all models\n",
    "input_size = real_len * s_rate       # sequence length\n",
    "output_size = max_M * (2 + dim)      # parameters matrix\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCM4YJnUw1w7",
    "tags": []
   },
   "source": [
    "### General\n",
    "Some general global variables. The important one is `ts` that holds the global time stamps vector in use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "oglSFZDuw3-2"
   },
   "outputs": [],
   "source": [
    "models = [\"CNN\", \"LSTM\", \"LSTM bi\", \"BOF\"]\n",
    "\n",
    "# losses options are described in the Loss section\n",
    "losses = [\"V\", \"P\", \"V+\", \"P+\", \"BOF\"]\n",
    "\n",
    "# A generic time stamps vector in the size of sequence length\n",
    "ts = torch.linspace(0, real_len, s_rate * real_len).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD3yMQc7n6_6",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **Utilities**\n",
    "*Signal utilities:* Filtering, Velocity Profiles Reconstruction, Minimum-Jerk curves calculation, and Plot helper.\n",
    "\n",
    "*pytorch utilities:* Flattening layer, Dimensions-print layer, Clipping layer, and Dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Signal utilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hy3ruio7qoXF",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Filter Signal\n",
    "Filtering velocity profiles vectors.\n",
    "The smoothed profile should reflect a more natural human movement.\n",
    "\n",
    "The function handles either batch filtering or a single sample filtering.\n",
    "\n",
    "**Input shape: either** `(N, d, len)` **or** `(d, len)`, where `N` is batch size, `d` is the spatial dimension and `len` in sequence length.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "zWiWfeSAq08q"
   },
   "outputs": [],
   "source": [
    "from scipy.signal import filtfilt, butter\n",
    "\n",
    "def filter_signal(vel, time=ts):\n",
    "    filter_size = torch.rand(1) + filter_size_mean - 0.5    # filter size is ~filter_size_mean\n",
    "    dt = torch.median(torch.diff(time))\n",
    "    b, a = butter(2, filter_size / ((1 / dt) / 2))\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    b = torch.tensor(b, dtype=torch.float32)\n",
    "    a = torch.tensor(a, dtype=torch.float32)\n",
    "\n",
    "    filtered_signal = filtfilt(b, a, vel.cpu().numpy())\n",
    "    \n",
    "    return torch.from_numpy(filtered_signal.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsHESajar7ta",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Batch Reconstruction\n",
    "Given a batch of sub movements parameters, reconstruct the velocity profiles. Note that the batch is the output from the model.\n",
    "\n",
    "__Input shape__: `(N, max_M * (d+2))`, where `N` is batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "R_IkHN7bsCZ2"
   },
   "outputs": [],
   "source": [
    "def batch_reconstruct_prof(batch):\n",
    "    batch = batch.view(batch.shape[0], max_M, dim + 2)\n",
    "    \n",
    "    T = batch[:, :, 0]\n",
    "    D = batch[:, :, 1]\n",
    "    A = batch[:, :, 2:]\n",
    "    min_jerk_profiles = minimum_jerk_velocity(T.unsqueeze(-1), D.unsqueeze(-1), A, ts, True)\n",
    "\n",
    "    return torch.sum(min_jerk_profiles, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaN8VynLpgjG",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Minimum Jerk Curve\n",
    "Given a Minimum Jerk parameters (Time, Duration, Amplitudes), return a polynomial curve described here:\n",
    "\n",
    "Flash and Hogan (1985):\n",
    "https://www.jneurosci.org/content/5/7/1688.short\n",
    "\n",
    "**Input shapes:** \n",
    "\n",
    "*if* `batch=True`:\n",
    "\n",
    "***start_t*** & ***duration:*** `(N, max_M)`, where `N` is batch size.\n",
    "***amplitudes:*** `(N, max_M, d)`, where `d` is the spatial dimension.\n",
    "\n",
    "*if* `batch=False`:\n",
    "\n",
    "***start_t*** & ***duration:*** `(max_M,)`\n",
    "***amplitudes:*** `(max_M, d)`, where `d` is the spatial dimension.\n",
    "\n",
    "*The firsrt cell is old, and should be carefully reased.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "DBVPbwpUpd2_",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # OLD VERSION - need to be erased safely\n",
    "# def _minimum_jerk_velocity_1D(start_t, duration, amplitudes, t = ts):\n",
    "\n",
    "#     d = len(amplitudes)\n",
    "#     displacements = []\n",
    "    \n",
    "#     # normalise time to t0 and movement duration, take only the time of the movement\n",
    "#     normalized_time = (t - start_t) / duration\n",
    "#     logical_movement = (normalized_time >= 0) & (normalized_time <= 1)\n",
    "\n",
    "#     # normalize displacements\n",
    "#     for amp in amplitudes:\n",
    "#         displacements.append(amp / duration)\n",
    "\n",
    "#     displacements = Tensor(displacements).reshape(d, 1)\n",
    "\n",
    "#     # initializes velocities\n",
    "#     vel = torch.zeros((t.shape[0], d))\n",
    "    \n",
    "#     # calculate velocities\n",
    "#     def min_jerk_polynomial(base_val):\n",
    "#         # the polynomial function from Flash and Hogan (1985)\n",
    "#         return base_val * (-60*normalized_time[logical_movement]**3 + 30*normalized_time[logical_movement]**4 + 30*normalized_time[logical_movement]**2)\n",
    "\n",
    "#     vel[logical_movement, :] = min_jerk_polynomial(displacements).T\n",
    "    \n",
    "#     return vel.permute(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_jerk_velocity(start_t, duration, amplitudes, t = ts, batch=True, eps=1e-12):\n",
    "    \n",
    "    duration[duration == 0] = eps    # preventing division-by-zero\n",
    "\n",
    "    max_M, d = amplitudes.shape[-2:]\n",
    "    \n",
    "    # normalise time to t0 and movement duration, take only the time of the movement\n",
    "    normalized_time = (t - start_t) / duration\n",
    "    logical_movement = ((normalized_time >= 0) & (normalized_time <= 1)).long()\n",
    "\n",
    "    displacements = (amplitudes / duration)\n",
    "    displacements = displacements[:, :, :, None] if batch else displacements[:, :, None]\n",
    "    \n",
    "    # the polynomial function from Flash and Hogan (1985)\n",
    "    norm_time = normalized_time * logical_movement\n",
    "    region = norm_time[:, :, None, :] if batch else norm_time[:, None, :]\n",
    "    min_jerk_profiles = displacements * (-60*region**3 + 30*region**4 + 30*region**2)\n",
    "\n",
    "    return min_jerk_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMRBUvhDrEak",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Sample Plot**\n",
    "Plot velocity profile with sub movements decomposition\n",
    "\n",
    "atm ONLY support 1-d movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "gcOB8cJWrGuc"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_sample(param, M=max_M, ts=ts, real_vel=None, axs=None, show=False, legend=True):\n",
    "    \n",
    "    param = param.detach().view(max_M,2+dim)\n",
    "    pred_M = calculate_M(param, batch=False)\n",
    "    \n",
    "    if not axs:\n",
    "        fig, axs = plt.subplots(1, 1)\n",
    "\n",
    "    # plot the sub movements and the reconstructed profile\n",
    "    vel = torch.zeros((ts.shape[0],1))\n",
    "    for i in range(M):\n",
    "        T, D, A = param[i, 0], param[i, 1], param[i, 2:]\n",
    "        prof = _minimum_jerk_velocity_1D(T, D, A, ts)\n",
    "        vel += prof\n",
    "        axs.plot(ts, prof, linestyle=':', linewidth=0.7)\n",
    "        \n",
    "    axs.plot(ts.detach(), vel.view(-1), 'gray', linewidth=0.8, label=\"Reconstructed vel.\")\n",
    "    axs.set_xlabel(\"pred M: {}\".format(pred_M))\n",
    "    axs.set_ylim([-(amp_duration_ratio*2), amp_duration_ratio*2])\n",
    "\n",
    "    # plot the real velocity profile if given\n",
    "    if real_vel is not None:\n",
    "        axs.plot(ts.detach(), real_vel.detach().view(-1), 'red', linewidth=0.9, label=\"Real vel.\")\n",
    "    if legend:\n",
    "        axs.legend()\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPpmhQ0Hqajv",
    "tags": []
   },
   "source": [
    "## **Pytorch utilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4ri1SE9leT-",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Flatten module, to use in nn.Sequencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "-xH1r9ZGqtNv"
   },
   "outputs": [],
   "source": [
    "class FlattenBatch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenBatch, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x.flatten(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9XLhGavljLw",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Printer module, to use in nn.Sequencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "zI33EOHclod6"
   },
   "outputs": [],
   "source": [
    "class PrintDim(nn.Module):\n",
    "    def __init__(self, to_print=True, print_data=False):\n",
    "        super(PrintDim, self).__init__()\n",
    "        self.to_print = to_print\n",
    "        self.print_data = print_data\n",
    "    def forward(self, x):\n",
    "        if self.print_data:\n",
    "            print(\"input: \", x)\n",
    "        if self.to_print:\n",
    "            print(\"dimensions: \", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Clipping Layer\n",
    "To clip infeasible values from the model's outputs.\n",
    "\n",
    "**input shape:** `(N, max_M * (d+2))` where `N` is the batch size\n",
    "\n",
    "**output shape:**  same as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipOutput(nn.Module):\n",
    "    def __init__(self, min_duration, min_amp):\n",
    "        super(ClipOutput, self).__init__()\n",
    "        self.min_D = min_duration\n",
    "        self.min_amp = min_amp\n",
    "\n",
    "    def forward(self, x):        \n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, dim + 2)\n",
    "        \n",
    "        time_mask = (x[:, :, 0] < 0)\n",
    "        duration_mask = (x[:, :, 1] < self.min_D)\n",
    "        amp_mask = (torch.abs(x[:, :, 2:]) < self.min_amp).any(dim=2)\n",
    "        mask = (time_mask | duration_mask | amp_mask).unsqueeze(-1)\n",
    "        x = x * (~mask)\n",
    "        \n",
    "        return x.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0Mj3HXdpVzm",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Errors definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Z5tiug_IpZad",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AmountPerSecondsViolation(Exception):\n",
    "    def __init__(self, message=\"Violation of limitation: Amount of submovements per second. This movement was stashed\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IGTI5AR1rDH",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Dataloader\n",
    "Given path to data objects, list of M (amount of SMs) and percentages of training set, return train dataloader and validation dataloader instaces, shuffled and with the appropriate batch size.\n",
    "\n",
    "Insures that the data will be in the shape:\n",
    "\n",
    "* `velocities.shape = ([N * M, real_len * s_rate])`\n",
    "* `parameters.shape = ([N * M, max_M * (2+d)])`\n",
    "* `amounts.shape = ([N * M])` \n",
    "\n",
    "Where: `M` is the length of the list of M argument, `max_M` is maximal SMs, `d` is the spatial dimension, `s_rate` is the sample rate, and `real_len` is the sequence's length in seconds. All are defined in the parameters section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubMovDataloader(Dataset):\n",
    "    def __init__(self, velocities, amounts, parameters):\n",
    "        if len(velocities.shape) == 2:\n",
    "            velocities = velocities[:,None,:]\n",
    "        \n",
    "        self.velocities = velocities\n",
    "        self.amounts = amounts\n",
    "        self.parameters = parameters.nan_to_num()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.amounts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vel = self.velocities[idx]\n",
    "        M = self.amounts[idx]\n",
    "        param = self.parameters[idx]\n",
    "        return vel, M, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_np_data(vel_path, M_path, param_path, batch_size, M_lst: list = [], train: float = 0.8):\n",
    "    velocities = Tensor(np.load(vel_path)).permute(0,2,1)\n",
    "    amounts = Tensor(np.load(M_path))\n",
    "    parameters = torch.flatten(Tensor(np.load(param_path)), start_dim=1)\n",
    "    \n",
    "    return load_data(velocities, amounts, parameters, batch_size, M_lst, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_torch_data(vel_path, M_path, param_path, batch_size, M_lst: list = [], train: float = 0.8):\n",
    "    velocities = torch.load(vel_path)\n",
    "    amounts = torch.load(M_path)\n",
    "    parameters = torch.load(param_path)\n",
    "    \n",
    "    return load_data(velocities, amounts, parameters, batch_size, M_lst, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "dOooVIXY1u4i"
   },
   "outputs": [],
   "source": [
    "def load_data(velocities, amounts, parameters, batch_size, M_lst: list = [], train: float = 0.8):\n",
    "\n",
    "    N = len(amounts) // (max_M-1)   # How many samples per each value of M\n",
    "    mask = torch.full((len(amounts),), True, dtype=bool)   # will mask the output to the desired M values\n",
    "\n",
    "    # take only desired values of M\n",
    "    if M_lst:\n",
    "        mask[:] = False\n",
    "        for M in M_lst:\n",
    "            if M < 2 or M > max_M:\n",
    "                raise ValueError(\"M_lst must contains values only in the range [2, max_M]\")\n",
    "            mask[(M-2)*N:(M-1)*N] = True\n",
    "    \n",
    "    # convert to Pytorch's Dataloader instance and split to train and validation\n",
    "    dataloader = SubMovDataloader(velocities[mask], amounts[mask], parameters[mask])\n",
    "    print(\"data size :\", amounts[mask].shape)\n",
    "    \n",
    "    train_set, val_set = torch.utils.data.random_split(dataloader, [train, 1 - train])\n",
    "    \n",
    "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_set, batch_size=1024, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(data_type=\"Big\", batch_size=64, M_lst=[], train_percentages=0.8):\n",
    "    # instansiate a dataloader with proper batch size\n",
    "    \n",
    "    data_path = root_dir + \"Dataset/%s\" % data_type + \"/{}.pt\"\n",
    "    vel_path, M_path, param_path = data_path.format('vel'), data_path.format('amounts'), data_path.format('param')\n",
    "    return load_torch_data(vel_path, M_path, param_path, batch_size, M_lst=M_lst, train=train_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as-NxyjapyY7",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **Dataset Generation**\n",
    "The shape of a dataset contains N samples for each M - amounts of SMs (All are torch.Tensor objects):\n",
    "* `velocities.shape = ([N * M-1, real_len * s_rate])`\n",
    "* `parameters.shape = ([N * M-1, M * (2+d)])`\n",
    "* `amounts.shape = ([N * M-1])` \n",
    "\n",
    "Where: `M` is `max_M`, `d` is the spatial dimension, `s_rate` is the sample rate, and `real_len` is the sequence's length in seconds. All are defined in the parameters section.\n",
    "\n",
    "*__The data was tested so far is only 1 dimensional. The code should work with multidimensional data, but this scenario hasn't been tested yet.__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The 1-d data realizability was tested with the optimization algorithm, as can be shown in the figure:*\n",
    "![alt text](Opt-Algorithm_on_Dataset.png)\n",
    "*The real profile is covered almost entirely by the reconstructed profile.*\n",
    "*It also shows that the algorithm take very long time to compute...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKzF0kfFljT7",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Sub movement generation\n",
    "The process of generating the sub movements parameters while maintaining the following restrictions:\n",
    "\n",
    "\n",
    "*   Fully-included SM in another one is not allowed (`T_i + D_i > T_i-1 + D_i-1`)\n",
    "*   At most 2 SMs overlapping at any time\n",
    "*   Prevent spikes in amplitudes (`amp_duration_ratio` parameter), as well as very low amplitudes (`amp_min` parameter)\n",
    "*   Most of the SMs are overlapping (`train_data_with_stops` is the percentages, `overlapping_range` is the area of overlap)\n",
    "*   Keep amplitudes in reasonable values, by limit the displacements results from the movements to be in the range of [-1, 1], represents a normalized work board\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nxo05gpdp51S"
   },
   "source": [
    "First sub movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "ptQMmTWjp7tZ"
   },
   "outputs": [],
   "source": [
    "def first_random_sub_movement(d: int = 1):\n",
    "    t_start = np.random.uniform(0., t_first_range)\n",
    "    duration = np.random.uniform(duration_min, duration_max)\n",
    "    amp = np.random.uniform(amp_min, min(amp_max, duration / amp_duration_ratio), size=(d,)) * \\\n",
    "          np.random.choice([-1, 1], size=(d,))  # directions of the movements\n",
    "    return Tensor(np.c_[t_start, duration, amp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI-UXF8Ap9SQ"
   },
   "source": [
    "All the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "BZ8X4t-Cp_ws"
   },
   "outputs": [],
   "source": [
    "def random_sub_movement(prev_T: float, prev_D: float, pos: np.ndarray, prev_prev_dt : float = 0.,\n",
    "                        amount_per_sec_prev_T: float = -np.inf, d: int = 1) -> np.ndarray:\n",
    "\n",
    "    # randomly decide whether this SM is overlapping or not\n",
    "    with_stops = np.random.randint(0,int(1/train_data_with_stops)) == 1\n",
    "    a, b = with_stops_range if with_stops else overlapping_range\n",
    "\n",
    "    # choose start time\n",
    "    s = max(prev_T, prev_prev_dt)       # prevent more than 2 overlapping submovements at any time\n",
    "    prev_dt = prev_T + prev_D - s\n",
    "    t_start = np.random.uniform(s + a*prev_dt, s + b*prev_dt)\n",
    "\n",
    "    # choose duration\n",
    "    min_D = max(duration_min, prev_T + prev_D - t_start)        # prevent fully-included submovements in another one\n",
    "    duration = np.random.uniform(min_D, duration_max)\n",
    "    if t_start + duration - amount_per_sec_prev_T < amount_per_seconds[\"seconds\"]:\n",
    "        return random_sub_movement(prev_T, prev_D, pos, prev_prev_dt, amount_per_sec_prev_T, d)\n",
    "\n",
    "    # choose amplitudes\n",
    "    # define the possible intervals for the amplitude in according to the current position\n",
    "    intervals = np.array([[((-amp_max-p)* duration, -amp_min),                  # spatial restriction to sum\n",
    "                           (amp_min, (amp_max-p)* duration)] for p in pos])     # the displacement to maximum of |1|\n",
    "    # Choose a direction\n",
    "    directions = np.random.choice([0, 1], size=intervals.shape[0])\n",
    "    # Generate random numbers based on the chosen intervals\n",
    "    amp = np.zeros(d)\n",
    "    for i in range(d):\n",
    "        rng = intervals[i][directions[i]]\n",
    "        amp[i] = np.random.uniform(rng[0], rng[1])\n",
    "        if np.abs(amp[i]) / duration > amp_duration_ratio:              # limit the ration between amplitude\n",
    "            amp[i] = np.sign(amp[i]) * amp_duration_ratio    # and duration to prevent high spikes\n",
    "\n",
    "    return Tensor(np.c_[t_start, duration, amp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGgOjYZMqdri",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Sample generation\n",
    "A single full-sample generation contains tuple of Tensors.\n",
    "\n",
    "Shape:\n",
    "`[vel, M, param]` Where :\n",
    "\n",
    "*   `vel.shape = (400,1)` represents 1-d velocity vector of 4 seconds (100Hz)\n",
    "*   `M.shape = (1,)` is the amount of sub movements created this velocity profile\n",
    "*   `param.shape = (max_M, 3)`, the parameters of the sub movements. If `M < max_M`, than `param[M:] = NaN`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saTxN9jTriUc"
   },
   "source": [
    "random parameters generation, given `m` amount of sub movements and `d` dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "jLzvWVXdqgsF"
   },
   "outputs": [],
   "source": [
    "def random_sub_movements_parameters(m: int, d: int = 1):\n",
    "    param = torch.zeros((m,2+d))\n",
    "    pos = torch.zeros((d,))\n",
    "\n",
    "    # first sub movement\n",
    "    param[0, :] = first_random_sub_movement(d = d)\n",
    "\n",
    "    prev_prev_dt = 0.\n",
    "    amount_per_sec_prev_T = -torch.inf\n",
    "\n",
    "    # build m-1 more sub movements\n",
    "    try:\n",
    "        for i in range(1, m):\n",
    "            last_displacement = param[i-1, 2:] / param[i-1, 1]      # displacement = A / D\n",
    "            pos += last_displacement                    # update the positions in according to the last displacement\n",
    "            param[i, :] = random_sub_movement(prev_T = param[i-1, 0], prev_D = param[i-1, 1], pos = pos,\n",
    "                                              prev_prev_dt = prev_prev_dt, amount_per_sec_prev_T = amount_per_sec_prev_T, d = d)\n",
    "            prev_prev_dt = param[i - 1, 0] + param[i - 1, 1]\n",
    "            if i >= amount_per_seconds[\"amount\"]-2:\n",
    "                amount_per_sec_prev_T = param[i-2, 0]\n",
    "    except RecursionError:\n",
    "        raise AmountPerSecondsViolation\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDeK5SXwrbmP"
   },
   "source": [
    "sample generation, summerizes the sub movement into a velocity profile.\n",
    "Parameters matrix is always of size `max_M x d`, and if `M < max_M`, than there will be `NaN` values in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "cx5XOVvRrz9j"
   },
   "outputs": [],
   "source": [
    "def build_sample(M: int, d: int = 1) -> tuple:\n",
    "    vel = torch.zeros((ts.shape[0], d))\n",
    "    param = random_sub_movements_parameters(M, d)\n",
    "    for i in range(M):\n",
    "        T, D, A = param[i, 0], param[i, 1], param[i, 2:]\n",
    "        prof = _minimum_jerk_velocity_1D(T, D, A)\n",
    "        vel += prof\n",
    "        \n",
    "    full_matrix = torch.empty((max_M, d+2))\n",
    "    full_matrix[:M] = param\n",
    "    full_matrix[M:] = torch.nan\n",
    "        \n",
    "    return vel[:real_len * s_rate, :], full_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l82dR-MsU6s",
    "tags": []
   },
   "source": [
    "## **Generate The Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "SkpvqQB-sYgE"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate_db(N, dim=1, path=\"{}\", plot=False):\n",
    "    print(\"Generating batch\")\n",
    "    \n",
    "    velocities = torch.empty(size=(dim,ts.shape[0]))\n",
    "    parameters = torch.empty(size=(1,max_M*(dim+2)))\n",
    "    amounts = []\n",
    "\n",
    "    fails, times = 0, []\n",
    "\n",
    "    # amount varies between 2 and max_M submovements\n",
    "    for m in range(2, max_M+1):\n",
    "        print(\"start M=\", m)\n",
    "        i = 0\n",
    "\n",
    "        # for each amount, create N samples\n",
    "        while i < N:\n",
    "            t = time.time()\n",
    "            try:\n",
    "                vel, param = build_sample(M=m, d=dim)\n",
    "                velocities = torch.row_stack((velocities,vel.permute(1, 0)))\n",
    "                parameters = torch.row_stack((parameters,param.view(1,-1)))\n",
    "                amounts.append(m)\n",
    "                i+=1\n",
    "                \n",
    "                if plot:\n",
    "                    plot_sample(param.view(max_M,dim+2), real_vel=vel)\n",
    "\n",
    "            except AmountPerSecondsViolation:\n",
    "                fails += 1\n",
    "            finally:\n",
    "                if i % 2000 == 1999:\n",
    "                    print(\"--------Progress:  \", ((m-1)*i) / ((max_M-1)*N))\n",
    "                times.append(time.time() - t)\n",
    "\n",
    "    print(\"--------Failures %: {}.     Average time: {}\".format(fails / len(times), torch.mean(Tensor(times))))\n",
    "    print(\"--------Total time: \", torch.sum(Tensor(times)))\n",
    "    \n",
    "    torch.save(velocities[1:], path.format('vel.pt'))\n",
    "    torch.save(parameters[1:], path.format('param.pt'))\n",
    "    torch.save(Tensor(amounts), path.format('amounts.pt'))\n",
    "\n",
    "    print(\"Done\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u6Ig52bk56b",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **Custom Loss Functions Definition**\n",
    "several loss function are defined here:\n",
    "1. `\"V\"` - Velocity cost loss function\n",
    "2. `\"P\"` - MSE on the parameters matrix\n",
    "\n",
    "additionally punishments are for:\n",
    "* Wrong prediction on M (calculation described in the helper function)\n",
    "* Unfeasible values for time, duration or amplitude\n",
    "\n",
    "*For the addition of punishment, concatenate `\"+\"` to the loss string*\n",
    "\n",
    "**NOTICE:** All losses except \"BOF\", recieve:\n",
    "`real_vel, real_P, real_M, pred_vel, pred_P`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate M of a given parameters matrix\n",
    "\n",
    "Equivalent operation as ClipOutput layer does, except here you can pass parameters vector at any shape and for a single sample also.\n",
    "\n",
    "**input shape:** \n",
    "\n",
    "*if* `batch-True`: either `(N, max_m*(d+2))` or `(N, max_M, d+2)`\n",
    "\n",
    "*if* `batch-False`: either `(max_m*(d+2),)` or `(max_M, d+2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "jlqjQDIipAI7"
   },
   "outputs": [],
   "source": [
    "def calculate_M(P, batch=True, amp_tol=0.005):\n",
    "    # return how many SM there are in a given Parameters matrix\n",
    "    # maintain the input dimension, compatible with either batch or single sample\n",
    "    # The ooperation is as follow:\n",
    "    # - zeros all rows that satisfies 1 or more conditions:\n",
    "    #      1) amplitudes < amp_tol\n",
    "    #      2) duration < min_duration\n",
    "    #      3) start < 0.\n",
    "    # - return number of rows that are not zeros\n",
    "    \n",
    "    if not batch:\n",
    "        if len(P.shape) == 1:\n",
    "            P = P.view(max_M,dim+2)\n",
    "        P[P[:,0] <= 0., :] = 0.                         # time\n",
    "        P[P[:,1] < duration_min, :] = 0.                # duration\n",
    "        P[torch.abs(P[:,2:])[:,0] <= amp_tol, :] = 0.   # amp\n",
    "        P = torch.sum(P, dim=1)\n",
    "        return torch.sum(P != 0)\n",
    "    \n",
    "    else:\n",
    "        N = P.shape[0]\n",
    "        if len(P.shape) == 3:\n",
    "            P = P.view(P.shape[0],max_M * (dim+2))        \n",
    "        cliper = ClipOutput(duration_min, amp_tol)\n",
    "        P = cliper(P).view(N, -1, dim+2)\n",
    "        \n",
    "        # Count the number of non-zero rows for each sample in the batch\n",
    "        non_zero_counts = torch.count_nonzero(P, dim=1)\n",
    "        return non_zero_counts[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Velocity Cost Function\n",
    "Here defined the basic cost function of the velocity profile, as described here:\n",
    "\n",
    "Friedman, & Finkbeiner, 2010:\n",
    "\n",
    "https://www.researchgate.net/profile/Jason-Friedman-3/publication/228566929_Temporal_dynamics_of_masked_congruence_priming_evidence_from_reaching_trajectories/links/09e4150e8f67308733000000/Temporal-dynamics-of-masked-congruence-priming-evidence-from-reaching-trajectories.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tan_vel(vel, batch=True):\n",
    "    # calculate the tangential velocity of a given velocity tensor of shape (batch, d, N) or (d, N) if batch=False\n",
    "    # for d = 1, the tengential velocity it's just the absolute value\n",
    "    \n",
    "    dim_to_sum = 1 if batch else 0\n",
    "    \n",
    "    sqr_vel = vel**2\n",
    "    sum_sqr_vel = torch.sum(sqr_vel, dim=dim_to_sum)\n",
    "    tan_vel = torch.sqrt(sum_sqr_vel + 1e-12)    # add a small value to prevent NaN derivative on 0\n",
    "\n",
    "    return tan_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def velocity_cost(real, pred):\n",
    "    # calculate the loss of the velocity profile, including a loss term \n",
    "    # to minimze the possibilities for overlapping curves that are on\n",
    "    # opposite directions\n",
    "    \n",
    "    sqr_err = (real-pred)**2\n",
    "    sum_sqr_err = torch.sum(sqr_err,dim=1)\n",
    "    \n",
    "    tan_err = (tan_vel(real) - tan_vel(pred))**2\n",
    "    \n",
    "    total_err = sum_sqr_err + tan_err\n",
    "\n",
    "    return torch.mean(torch.sum(total_err, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Additional Punishments\n",
    "punish M with CrossEntropy (treat as a classification problem), and infeasible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punishment(real_M, pred_P):\n",
    "        \n",
    "    # punish every entry of T < 0\n",
    "    time_punish = torch.sum(pred_P[: :, 0] < 0.)  \n",
    "\n",
    "    # punish every entry of D < duration_min\n",
    "    duration_punish = torch.sum(pred_P[: :, 1] < duration_min)   \n",
    "\n",
    "    # add a loss term on M value - a CrossEntropyLoss\n",
    "    real_M_hot_vec = nn.functional.one_hot(real_M.long(), num_classes=max_M+1).float()\n",
    "    pred_M_hot_vec = nn.functional.one_hot(calculate_M(pred_P).long(), num_classes=max_M+1).float()\n",
    "    M_punish = nn.CrossEntropyLoss()(real_M_hot_vec, pred_M_hot_vec)\n",
    "\n",
    "    # scaling the punishments accordinly to the dimensions\n",
    "    T_D_punishes = (time_punish + duration_punish + M_punish) / (pred_P.shape[0] * max_M)\n",
    "    M_punish = M_punish / max_M\n",
    "    \n",
    "    return T_D_punishes + M_punish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SGL(Sparse Group Lasso) definition\n",
    "SGL as described here:\n",
    "\n",
    "https://arxiv.org/abs/1607.00485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGL(W, lambda1, lambda2):\n",
    "    # Calculate L1 regularization on individual weights\n",
    "    l1_reg = lambda1 * torch.norm(W, p=1)\n",
    "    \n",
    "    # Calculate L2 regularization on groups of weights\n",
    "    group_reg = 0\n",
    "    for row in W:\n",
    "        group_reg += lambda2 * torch.norm(row, p=2)\n",
    "    \n",
    "    return l1_reg + group_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### \"P\" loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParametersLoss(nn.Module):\n",
    "    def __init__(self, with_punish=False):\n",
    "        super(ParametersLoss, self).__init__()\n",
    "        self.with_punish = with_punish\n",
    "        \n",
    "    def forward(self, real_P, pred_P, real_M): \n",
    "\n",
    "        # simple MSE on the parameters matrices\n",
    "        mse_loss = nn.MSELoss()(real_P, pred_P) \n",
    "        \n",
    "        # add punish\n",
    "        if self.with_punish:\n",
    "            \n",
    "            # real & pred are matrix of: (batch_size, max_M * dim+2), reshape to: (batch, M, d+2)\n",
    "            pred_P = pred_P.view(-1, max_M, dim+2)   \n",
    "            \n",
    "            punish = punishment(real_M, pred_P)\n",
    "            return mse_loss + 0.1 * punish\n",
    "         \n",
    "        return mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### \"V\" loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VelLoss(nn.Module):\n",
    "    def __init__(self, with_punish=False):\n",
    "        super(VelLoss, self).__init__()\n",
    "        self.with_punish = with_punish\n",
    "        \n",
    "    def forward(self, real_vel, real_P, real_M, pred_vel, pred_P):\n",
    "        \n",
    "        # simple MSE on the velocity profiles\n",
    "        mse_loss = velocity_cost(real_vel, pred_vel) \n",
    "        \n",
    "        # add punish\n",
    "        if self.with_punish:\n",
    "            \n",
    "            # real_P & pred_P are matrix of: (batch_size, max_M * dim+2), reshape to: (batch, M, d+2)\n",
    "            pred_P = pred_P.view(-1, max_M, dim+2)\n",
    "        \n",
    "            punish = punishment(real_M, pred_P)\n",
    "            return mse_loss + punish\n",
    "         \n",
    "        return mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### loss getter (generate pytorch criterion instance)\n",
    "\n",
    "Possible values are: `\"V\"`, `\"V+\"`, `\"P\"`, `\"P+\"`, `\"BOF\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "c1CD45g1lxfH"
   },
   "outputs": [],
   "source": [
    "def get_loss_function(loss_type=\"P\"):\n",
    "    if loss_type not in losses:\n",
    "        raise ValueError('{} is not a valid loss_type. \\\n",
    "                        loss_type must be one of: \\\n",
    "                        \"V\" / \"V+\" for velocity reconstruction error, \\\n",
    "                        \"P\" / \"P+\" for parameters MSE loss, \\\n",
    "                        or \"BOF\" for BoF MSE'.format(loss_type))  \n",
    "    \n",
    "    # reconstruction error\n",
    "    if loss_type == \"V\":\n",
    "        V_Loss = VelLoss()\n",
    "        loss = lambda real_vel, real_P, real_M, pred_vel, pred_P : V_Loss(real_vel, real_P, real_M, pred_vel, pred_P)\n",
    "    \n",
    "    # reconstruction error with punish for infeasible values of parameters\n",
    "    elif loss_type == \"V+\":\n",
    "        V_plus_Loss = VelLoss(with_punish=True)\n",
    "        loss = lambda real_vel, real_P, real_M, pred_vel, pred_P : V_plus_Loss(real_vel, real_P, real_M, pred_vel, pred_P)\n",
    "    \n",
    "    # parameters error\n",
    "    elif loss_type == \"P\":\n",
    "        P_Loss = ParametersLoss()\n",
    "        loss = lambda real_vel, real_P, real_M, pred_vel, pred_P : P_Loss(real_P, pred_P, real_M)\n",
    "    \n",
    "    # parameters error with punish for infeasible values\n",
    "    elif loss_type == \"P+\":\n",
    "        P_plus_Loss = ParametersLoss(with_punish=True)\n",
    "        loss = lambda real_vel, real_P, real_M, pred_vel, pred_P : P_plus_Loss(real_P, pred_P, real_M)\n",
    "    \n",
    "    # BoF error - simply mse loss for costume use in the BOF-training loop\n",
    "    elif loss_type == \"BOF\":\n",
    "        loss = nn.MSELoss()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **My-Models section:**\n",
    "In this section there are several models that I have implemented:\n",
    "\n",
    "1) CNN - a convolutional neural network\n",
    "2) LSTM - a long-short term memory (RNN)\n",
    "3) BoF - a bag of function, model that meant to decompose signals\n",
    "\n",
    "The Bof is potentailly be high-relevant for our task. I haven't manage to train it well; maybe because I only used my implementation of LSTM model as encoders, and tsai's models might perform better.\n",
    "\n",
    "*__Recommendation: The models here are not state-of-the art models, and you should only use the \"tsai's Models\" section.__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heVcSqVaqHvr",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    count = 0\n",
    "    for p in model.parameters():\n",
    "        count += len(p)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-kRcNFWrbFU",
    "tags": []
   },
   "source": [
    "### CNN\n",
    "\n",
    "The model architecture:\n",
    "\n",
    "\n",
    "*   **3 Convolutional layers** (Conv1d -> BN1d -> LeakuReLU)\n",
    "*   **MLP** containing 3 Fully Connected layers\n",
    "*   **W**: Fully Connected layer for group sparse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "ilBkEGruqNm3"
   },
   "outputs": [],
   "source": [
    "class SubMovCNN(nn.Module):\n",
    "    def __init__(self, feat=16, print=False):\n",
    "        super(SubMovCNN, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            # current size: 1 x 1 x 400\n",
    "            PrintDim(print),\n",
    "            nn.Conv1d(in_channels=1, out_channels=feat, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(feat*200),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # current size: 1 x feat x 200\n",
    "            PrintDim(print),\n",
    "            nn.Conv1d(in_channels=feat, out_channels=2*feat, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(2*feat*200),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # current size: 1 x 2*feat x 100\n",
    "            PrintDim(print),\n",
    "            nn.Conv1d(in_channels=2*feat, out_channels=4 * feat, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(2*feat*200),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # current size: 1 x 4*feat x 50\n",
    "            PrintDim(print),\n",
    "            FlattenBatch(),\n",
    "            nn.Linear(4 * feat * 50, 720),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(720, 100),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(100, 7 * 3),\n",
    "\n",
    "            PrintDim(print),\n",
    "        )\n",
    "\n",
    "        # the group-sparse term\n",
    "        self.W = nn.Linear(7*3, 7*3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W(self.seq(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LSTM\n",
    "Optional: bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubMovLSTM(nn.Module):\n",
    "    def __init__(self, input_size=dim*input_size, hidden_size=hidden_size,\n",
    "                      num_layers=num_layers, output_size=output_size, dropout=0.2, bidirectional=True):\n",
    "        super(SubMovLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout,\n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        self.bn = nn.BatchNorm1d(2 * hidden_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, 120),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(120, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # initializes of hidden states\n",
    "        h0 = torch.ones(self.directions*self.num_layers, x.shape[0], self.hidden_size)\n",
    "        c0 = torch.ones(self.directions*self.num_layers, x.shape[0], self.hidden_size)\n",
    "                \n",
    "        # pass through LSTM. notice that output contains only 1-directional pass\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # if bidrectional, should concatenate last pass from both sides, otherwise, just take lase time step\n",
    "        h_n = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1) if self.directions == 1 else out[:,-1,:]\n",
    "        \n",
    "        # batch normalization. perform only for batch bigger than 32 samples\n",
    "        out = self.bn(h_n) if x.shape[0] >= 32 else h_n\n",
    "        \n",
    "        # MLP\n",
    "        out = self.mlp(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **BoF (Bag of Functions)**\n",
    "As described here:\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0957417423002889/pdfft?md5=1a853aba33eba3da31f65906a0fd2a88&pid=1-s2.0-S0957417423002889-main.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Functions\n",
    "\n",
    "Some functions to demonstrate the BoF performance on dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callable_name(item):\n",
    "    if callable(item):\n",
    "        if hasattr(item, '__name__'):\n",
    "            return item.__name__\n",
    "    return str(item)\n",
    "\n",
    "####### Trends\n",
    "def constant(x, y, z, ts=ts):\n",
    "    return torch.ones(ts.shape) * x\n",
    "\n",
    "def lin(x, y, z, ts=ts):\n",
    "    return ts * x\n",
    "\n",
    "def poly_2(x, y, z, ts=ts):\n",
    "    return (x * ts**2) + (ts * y)\n",
    "\n",
    "def poly_3(x, y, z, ts=ts):\n",
    "    return (x * ts**3) + (y * ts**2) + (z * ts)\n",
    "\n",
    "def inv_expo_minus(x, y, z, ts=ts):\n",
    "    return x * (1 - torch.exp(-ts * y))\n",
    "\n",
    "def inv_expo(x, y, z, ts=ts):\n",
    "    return x * (1 - torch.exp(ts * y))\n",
    "\n",
    "def asinh(x, y, z, ts=ts):\n",
    "    return x * torch.asinh(y * (ts - z))\n",
    "\n",
    "def exp_trend(x, y, z, ts=ts):\n",
    "    return x * ((ts + y) ** z)\n",
    "\n",
    "def log_trend(x, y, z, ts=ts):\n",
    "    return x * torch.log(ts + y)\n",
    "\n",
    "\n",
    "####### Seasonality\n",
    "def sin(x, y, z, ts=ts):\n",
    "    return x * torch.sin(ts*y + z)\n",
    "\n",
    "def cosin(x, y, z, ts=ts):\n",
    "    return x * torch.cos(ts*y + z)\n",
    "\n",
    "def sinc(x, y, z, ts=ts):\n",
    "    return x * torch.sinc(ts*y + z)\n",
    "\n",
    "def rect(x, y, z, ts=ts):\n",
    "    return x * torch.sign(torch.sin(ts*y + z))\n",
    "\n",
    "\n",
    "####### Events\n",
    "def step(x, y, z, ts=ts):\n",
    "    return x * (y*ts > z).long()\n",
    "\n",
    "def exp_event(x, y, z, ts=ts):\n",
    "    return x * torch.exp(-y * (ts - z)**2)\n",
    "\n",
    "def tanh(x, y, z, ts=ts):\n",
    "    return x * torch.tanh(y * (ts - z))\n",
    "\n",
    "def sigm(x, y, z, ts=ts):\n",
    "    return x * torch.sigmoid(y * (ts - z))\n",
    "\n",
    "def min_jerk(T, D, A):\n",
    "    if D == 0:\n",
    "        D = eps\n",
    "\n",
    "    # normalise time to t0 and movement duration, take only the time of the movement\n",
    "    normalized_time = (t - T) / D\n",
    "    logical_movement = (normalized_time >= 0) & (normalized_time <= 1)\n",
    "\n",
    "    displacements = Tensor([A/D]).reshape(1,1)\n",
    "\n",
    "    # initializes velocities\n",
    "    vel = torch.zeros((t.shape[0], 1))\n",
    "    \n",
    "    # calculate velocities\n",
    "    def min_jerk_polynomial(base_val):\n",
    "        # the polynomial function from Flash and Hogan (1985)\n",
    "        return base_val * (-60*normalized_time[logical_movement]**3 + 30*normalized_time[logical_movement]**4 + 30*normalized_time[logical_movement]**2)\n",
    "\n",
    "    vel[logical_movement, :] = min_jerk_polynomial(displacements).T\n",
    "    \n",
    "    return vel.flatten()\n",
    "\n",
    "\n",
    "min_jerk_bof = [min_jerk]\n",
    "small_bof = [sin, constant, tanh]\n",
    "full_bof = [exp_event, tanh, sigm,\n",
    "       sin, cosin, sinc,\n",
    "       constant, lin, poly_2, poly_3,\n",
    "       inv_expo_minus, inv_expo, asinh, exp_trend, log_trend]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Dummy dataset for BoF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOFDataloader(Dataset):\n",
    "    def __init__(self, signals):\n",
    "        self.signals = signals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.signals[idx]\n",
    "        return s\n",
    "    \n",
    "    \n",
    "def bof_dummy_data_gen(bof, N=10000):\n",
    "    parameters = torch.rand(size=(N, 3))\n",
    "    weights = torch.rand(size=(N, len(bof)))\n",
    "    data = torch.stack([torch.sum(torch.stack([func(*params) * weight for func, weight in zip(bof, weights)]), dim=0)\n",
    "                          for params, weights in zip(parameters, weights)])\n",
    "    return data[:,None,:]\n",
    "    \n",
    "    \n",
    "def BOFdummyDataset(bof, N=10000):\n",
    "    dataloader = BOFDataloader(bof_dummy_data_gen(bof, N))\n",
    "    \n",
    "    train_set, val_set = torch.utils.data.random_split(dataloader, [0.8, 0.2])\n",
    "    \n",
    "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, feat=16, num_layers=3, to_print=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        c_in = 1 \n",
    "        modules = []\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            c_out = (2**i)*feat\n",
    "            \n",
    "            modules.append(PrintDim(to_print))\n",
    "            modules.append(nn.Conv1d(in_channels=c_in, out_channels=c_out,\n",
    "                                     kernel_size=4, stride=2, padding=1))\n",
    "            modules.append(nn.BatchNorm1d(c_out))\n",
    "            modules.append(nn.ReLU())\n",
    "            \n",
    "            c_in = c_out\n",
    "        \n",
    "        modules.append(FlattenBatch())\n",
    "        modules.append(PrintDim(to_print))\n",
    "        self.seq = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        print((self.seq(x)).shape)\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Heades - MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadMLP(nn.Module):\n",
    "    def __init__(self, c_in, c_out, to_print=False):\n",
    "        super(HeadMLP, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(c_in, 100),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(100, 100),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(100, c_out),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "BoF Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoFBlock(nn.Module):\n",
    "    def __init__(self, bof, top_k=1, input_len=input_size, encoder_feat=16, encoder_num_layers=3, to_print=False):\n",
    "        super(BoFBlock, self).__init__()\n",
    "        \n",
    "        # input size: 64 x 1 x 400\n",
    "        # self.encoder = EncoderCNN(encoder_feat, encoder_num_layers, to_print=to_print)\n",
    "        \n",
    "        self.encoder = SubMovLSTM(hidden_size=64, num_layers=3, output_size=3200)\n",
    "        encoder_out_features = (2**(encoder_num_layers-2))*encoder_feat if encoder_num_layers > 1 else 1\n",
    "        encoder_out_size = input_len / (2**(encoder_num_layers-1))\n",
    "        self.heads_in_size = int(encoder_out_features * encoder_out_size // 2)\n",
    "        \n",
    "        self.selection_head = HeadMLP(self.heads_in_size, len(bof), to_print=to_print) # outputs weight each function from BoF\n",
    "        self.parameters_head = HeadMLP(self.heads_in_size, 2+dim, to_print=to_print)   # 2+dim = amount of parameters per SM\n",
    "        \n",
    "        self.bof = bof\n",
    "        self.K = top_k\n",
    "\n",
    "    def forward(self, x, return_selections=False):\n",
    "        x = self.encoder(x)\n",
    "        selection = self.selection_head(x[:,:self.heads_in_size])\n",
    "        parameters = self.parameters_head(x[:,self.heads_in_size:])\n",
    "        \n",
    "        # perform softamx only on the top K weights and zero the others\n",
    "        topk_indices = torch.topk(selection, self.K, dim=1).indices\n",
    "        mask = torch.zeros_like(selection)\n",
    "        mask.scatter_(1, topk_indices, selection.gather(1, topk_indices))\n",
    "        values_for_softmax = selection[mask > 0].view(x.shape[0],self.K)\n",
    "        softamxed = F.softmax(values_for_softmax, dim=1).flatten()\n",
    "        bool_mask = torch.argwhere(mask > 0)\n",
    "        mask[bool_mask[:,0], bool_mask[:,1]] = softamxed\n",
    "        selection = mask\n",
    "        \n",
    "        # sum the parameterized functions\n",
    "        out = torch.stack([torch.sum(torch.stack([func(*params) * w for func, w in zip(self.bof, weights)]), dim=0) \n",
    "                           for params, weights in zip(parameters, selection)])\n",
    "        \n",
    "        if return_selections:\n",
    "            return out[:,None,:], selection\n",
    "        return out[:,None,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoF(nn.Module):\n",
    "    def __init__(self, num_blocks, input_len=input_size,\n",
    "                 encoder_feat=16, encoder_num_layers=3, to_print=False):\n",
    "        super(BoF, self).__init__()\n",
    "        \n",
    "        self.bof = min_jerk_bof\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            block = BoFBlock(bof=self.bof, top_k=1, input_len=input_len, encoder_feat=encoder_feat,\n",
    "                             encoder_num_layers=encoder_num_layers, to_print=to_print)\n",
    "            self.blocks.append(block)\n",
    "            \n",
    "    def forward(self, x, plot=False):\n",
    "        residual = x\n",
    "        if plot:\n",
    "            plt.plot(ts, x[0][0].detach(), label=\"true\")\n",
    "        summary = torch.zeros(x.shape)\n",
    "        for block in self.blocks:\n",
    "            out = block(residual)\n",
    "            if plot:\n",
    "                plt.plot(ts, out[0][0].detach())\n",
    "            summary = summary + out\n",
    "            residual = residual - out\n",
    "        if plot:\n",
    "            plt.plot(ts, summary[0][0].detach(), linestyle=\":\", label=\"pred\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        return summary, residual\n",
    "\n",
    "    def interpret(self, x):\n",
    "        # return: output, residual, list of len(blocks) Strings \n",
    "        # represent the decomposition in each block\n",
    "        \n",
    "        interpretation = torch.empty(size=(x.shape[0], len(self.blocks), len(self.bof)))\n",
    "        residual = x\n",
    "        summary = torch.zeros(x.shape)\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out, selections = block(residual, return_selections=True)\n",
    "            summary = summary + out\n",
    "            residual = residual - out\n",
    "            interpretation[:, i] = selections\n",
    "        \n",
    "        result_strings = []\n",
    "        for ba in range(x.shape[0]):\n",
    "            string = \"\"\n",
    "            for bl in range(len(self.blocks)):\n",
    "                string += \"Block {}: \".format(bl+1)\n",
    "                for i in range(len(self.bof)):\n",
    "                    string += \"{} x {}, \".format(get_callable_name(self.bof[i]), interpretation[ba][bl][i])\n",
    "                string = string[:-2] + \"\\n\"\n",
    "            result_strings.append(string)\n",
    "        \n",
    "        return summary, residual, result_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    model_type = config.model_type\n",
    "    if model_type not in models:\n",
    "        raise ValueError(\"{} is not a valid model type. type should be one of: {}\".format(model_type, models))\n",
    "\n",
    "    if model_type == \"CNN\":\n",
    "        return SubMovCNN()\n",
    "\n",
    "    elif model_type == \"LSTM\":\n",
    "        return SubMovLSTM(hidden_size=config.LSTM_hidden, num_layers=config.LSTM_layers, bidirectional=False)\n",
    "    \n",
    "    elif model_type == \"LSTM bi\":\n",
    "        return SubMovLSTM(hidden_size=config.LSTM_hidden, num_layers=config.LSTM_layers, bidirectional=True)\n",
    "    \n",
    "    elif model_type == \"BOF\":\n",
    "        return BoF(num_blocks=config.num_blocks, encoder_feat=config.encoder_feat, encoder_num_layers=config.encoder_num_layers, to_print=config.to_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYDl5U4x0YIc",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9N28GeS20aUS"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, validation_set, loss_function, config, plot=True):\n",
    "    with torch.no_grad():\n",
    "        avg_loss = 0.\n",
    "        M_accuracy = 0.\n",
    "        for i, (batch_vel, batch_M, batch_param) in enumerate(validation_set):\n",
    "            batch_vel = filter_signal(batch_vel.float(), ts) if config.filter_signal else batch_vel.float()\n",
    "\n",
    "            # predict parameters\n",
    "            pred_param = model(batch_vel)\n",
    "\n",
    "            # calculate M prediction accuracy\n",
    "            pred_M = calculate_M(pred_param)\n",
    "            M_correctness = pred_M == batch_M\n",
    "            M_accuracy += torch.sum(M_correctness) / len(batch_M)\n",
    "\n",
    "            # calculate predicted velocity profile\n",
    "            pred_vel = batch_reconstruct_prof(torch.clone(pred_param)).float() if config.loss_type in [\"V\", \"V+\"] else None\n",
    "\n",
    "            # calculate MSE loss between real and predicted velocity profiles\n",
    "            loss = loss_function(batch_vel.float(), batch_param.float(), batch_M, pred_vel, pred_param.float())\n",
    "            avg_loss += loss.item()\n",
    "        \n",
    "            # plot the first sample recondtruction\n",
    "            if plot:\n",
    "                fig, axs = plt.subplots(3,3)\n",
    "                for i in range(9):\n",
    "                    ax = axs[i%3][i//3]\n",
    "                    plot_sample(pred_param[i], real_vel=batch_vel[i], axs=ax, legend=False)\n",
    "                    ax.set_yticks([])\n",
    "                    ax.set_xticks([])\n",
    "                fig.suptitle(\"Red = Real.   Dotted black = Predicted\")\n",
    "                plt.show()\n",
    "                plot = False\n",
    "\n",
    "    print(\"****************** epoch done********* . loss: \", avg_loss / len(validation_set), \"M acc: \", M_accuracy.item() / len(validation_set))\n",
    "    # wandb.log({\"Accuracy of pred M\": M_accuracy / len(validation_set)})\n",
    "    return avg_loss / len(validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJY7Ed3QwJ4b",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Training**\n",
    "Should recieve `config` object with the hyper-parameters:\n",
    "\n",
    "*   `loss_type`         (\"V\", \"V+\", \"P\", \"P+\")\n",
    "*   `model_type`        (\"CNN\", \"LSTM\", \"LSTM bi\", \"BOF\")\n",
    "*   `lr`\n",
    "*   `l1_reg`\n",
    "*   `sgl_reg`\n",
    "*   `batch_size`\n",
    "*   `LSTM_layers`       (if model is a LSTM)\n",
    "*   `LSTM_hidden`       (if model is a LSTM)\n",
    "*   `num_blocks`        (if model is a BOF)\n",
    "*   `encoder_feat`      (if model is a BOF)\n",
    "*   `encoder_num_layers`(if model is a BOF)\n",
    "*    `M_lst`            \n",
    "*    `filter_signal`    (if True, the signal will be filtered before forward to model)\n",
    "*    `to_print`         (if True, model will print dimension in the forward pass)\n",
    "*    `dataset`          (\"Big\" or \"Small\")\n",
    "\n",
    "\n",
    "*Data shape:*\n",
    "* `velocities.Size = ([N * M, d, real_len * s_rate])`\n",
    "* `parameters.Size = ([N * M, max_M * (2+d)])`\n",
    "* `amounts.Size = ([N * M])`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "WB_Fd8r_wOT8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(config=None, q=50):\n",
    "    # with wandb.init(config=config):\n",
    "    #     config = wandb.config\n",
    "\n",
    "        train, val = get_data_loader(data_type=config.dataset, batch_size=config.batch_size,\n",
    "                                     M_lst=config.M_lst, train_percentages=0.8)\n",
    "\n",
    "        # Create an instance of the model\n",
    "        SM1D = get_model(config)\n",
    "        print(\"parameters: \",count_parameters(SM1D))\n",
    "        optimizer = optim.Adam(SM1D.parameters(), lr=config.lr)\n",
    "        # NOTICE: the loss recieves: (real_val, real_P, real_M, pred_vel, pred_P)\n",
    "        criterion = get_loss_function(config.loss_type)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            epoch_train_loss = 0.\n",
    "            running_loss = 0.\n",
    "\n",
    "            for batch_idx, (batch_vel, batch_M, batch_param) in enumerate(train):\n",
    "                batch_vel = filter_signal(batch_vel.float(), ts) if config.filter_signal else batch_vel.float()\n",
    "\n",
    "                # get prediction of SMs parameters\n",
    "                pred_param = SM1D(batch_vel)\n",
    "            \n",
    "                # recontruct velocity profiles for the loss function\n",
    "                pred_vel = batch_reconstruct_prof(torch.clone(pred_param)).float() if config.loss_type in [\"V\", \"V+\"] else None\n",
    "\n",
    "                # calculate the loss\n",
    "                loss = criterion(batch_vel.float(), batch_param.float(), batch_M, pred_vel, pred_param.float())\n",
    "                \n",
    "                # calculate M prediction accuracy\n",
    "                non_zero_counts = torch.count_nonzero(pred_param.view(batch_vel.shape[0], -1, dim+2), dim=1)\n",
    "                pred_M = non_zero_counts[:,0]\n",
    "                M_correctness = pred_M == batch_M\n",
    "                M_accuracy = torch.sum(M_correctness) / len(batch_M)\n",
    "\n",
    "                # calculate the SGL regularization on the MLP part of the model\n",
    "                # if config.l2_reg == 0, than this is just a regular L1 regularization\n",
    "                sgl_reg = 0.\n",
    "                for layer in SM1D.mlp:\n",
    "                    if isinstance(layer, nn.Linear):\n",
    "                        sgl_reg += SGL(layer.weight, config.sgl_reg, config.l2_reg)\n",
    "                loss += sgl_reg\n",
    "\n",
    "                # Perform the backward pass and optimization step\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                epoch_train_loss += loss.item()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                if batch_idx % q == q-1:  # print progress\n",
    "                    print(f'[{epoch + 1}, {batch_idx + 1:5d}] loss: {running_loss / q:.6f}. Accuracy: {M_accuracy}')\n",
    "                    running_loss = 0.\n",
    "                    \n",
    "            SM1D.eval()\n",
    "            val_loss = evaluate(SM1D, val, criterion, config, plot=False)\n",
    "            SM1D.train()\n",
    "            # wandb.log({\"Train loss\": epoch_train_loss / len(train_big), \"Test loss\": val_loss})\n",
    "\n",
    "            epoch_train_loss = 0.\n",
    "                    \n",
    "            torch.save(SM1D.state_dict(), '{}_M{}.pt'.format(config.model_type, config.M_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## BoF Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoF_eval(model, validation_set, criterion, plot=False):\n",
    "    with torch.no_grad():\n",
    "        avg_loss = 0.\n",
    "        M_accuracy = 0.\n",
    "        for i, (vel, m, param) in enumerate(validation_set):\n",
    "            vel = vel.float()\n",
    "            \n",
    "            pred = model(vel)[0]\n",
    "            loss = criterion(vel, None, None, pred, None)\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            # plot the first sample recondtruction\n",
    "            if plot:\n",
    "                fig, axs = plt.subplots(3,3)\n",
    "                for i in range(9):\n",
    "                  ax = axs[i%3][i//3]\n",
    "                  ax.plot(ts, vel[i][0], 'red')\n",
    "                  ax.plot(ts, pred[i][0], linestyle=':', linewidth=0.7)\n",
    "                  ax.set_yticks([])\n",
    "                  ax.set_xticks([])\n",
    "                fig.suptitle(\"Red = Real.   Dotted black = Predicted\")\n",
    "                plt.show()\n",
    "                plot = False\n",
    "\n",
    "        print(\"****************** epoch done********* . loss: \", avg_loss / len(validation_set))\n",
    "        # wandb.log({\"Accuracy of pred M\": M_accuracy / len(validation_set)})\n",
    "        return avg_loss / len(validation_set)\n",
    "\n",
    "\n",
    "def BoF_train(config, q=50):\n",
    "            \n",
    "        train, val = get_data_loader(data_type=config.dataset, batch_size=config.batch_size,\n",
    "                                     M_lst=config.M_lst, train_percentages=0.8)\n",
    "        \n",
    "        # Create an instance of the model\n",
    "        SM1D = get_model(config)\n",
    "\n",
    "        optimizer = optim.Adam(SM1D.parameters(), lr=config.lr, weight_decay=config.l2_reg)\n",
    "        criterion = get_loss_function(config.loss_type)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            epoch_train_loss = 0.\n",
    "            running_loss = 0.\n",
    "\n",
    "            for batch_idx, (vel, m, param) in enumerate(train):\n",
    "                vel = vel.float()\n",
    "                \n",
    "                pred, residual = SM1D(vel, plot=True)\n",
    "                \n",
    "                # calculate the loss\n",
    "                loss = criterion(vel, None, None, pred, None)\n",
    "                \n",
    "                # l1 reg\n",
    "                l1_reg = sum(torch.linalg.norm(p, 1) for p in SM1D.parameters())\n",
    "                loss += l1_reg * config.sgl_reg\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                epoch_train_loss += loss.item()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if batch_idx % q == q-1:  # print progress\n",
    "                    print(f'[{epoch + 1}, {batch_idx + 1:5d}] loss: {running_loss / q:.10f}')\n",
    "                    running_loss = 0.\n",
    "\n",
    "            SM1D.eval()\n",
    "            val_loss = BoF_eval(SM1D, val, criterion, plot=True)\n",
    "            SM1D.train()\n",
    "            print({\"Train loss\": epoch_train_loss / len(train), \"Test loss\": val_loss})\n",
    "            # wandb.log({\"Train loss\": epoch_train_loss / len(train_big), \"Test loss\": val_loss})\n",
    "\n",
    "            epoch_train_loss = 0.\n",
    "            \n",
    "        torch.save(SM1D.state_dict(), \"MinJerk_BoF.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcDcQcdNnGZo",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Runner Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_db(N=100000, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, model_type, loss_type, epochs, batch_size, lr, l2_reg, l1_reg, LSTM_layers, LSTM_hidden,\n",
    "                 num_blocks, feat, encoder_layers, to_print, M_lst = [], filter_signal=False, dataset=\"Big\"):\n",
    "        self.model_type = model_type\n",
    "        self.loss_type = loss_type\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.l2_reg = l2_reg\n",
    "        self.sgl_reg = l1_reg\n",
    "        self.LSTM_layers=LSTM_layers\n",
    "        self.LSTM_hidden=LSTM_hidden\n",
    "        self.num_blocks = num_blocks\n",
    "        self.encoder_feat = feat\n",
    "        self.encoder_num_layers = encoder_layers\n",
    "        self.M_lst = M_lst\n",
    "        self.filter_signal = filter_signal\n",
    "        self.to_print = to_print\n",
    "        self.dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "config = Config(model_type=\"LSTM bi\", loss_type=\"P\", epochs=15, batch_size=128, lr=0.05, l2_reg=0.0001,\n",
    "                  l1_reg=0., LSTM_layers=3, LSTM_hidden=64, num_blocks=7, feat=16, encoder_layers=2, \n",
    "                  to_print=False, M_lst=[], filter_signal=False, dataset=\"Big\")\n",
    "\n",
    "M_classifier_train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "config = Config(model_type=\"LSTM bi\", loss_type=\"P+\", epochs=15, batch_size=128, lr=0.05, l2_reg=0.0001,\n",
    "                  l1_reg=0., LSTM_layers=3, LSTM_hidden=64, num_blocks=7, feat=16, encoder_layers=2, \n",
    "                  to_print=False, M_lst=[2,3,4], filter_signal=False, dataset=\"Big\")\n",
    "\n",
    "train_loop(config, q=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train BoF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "M_lst = [4]\n",
    "\n",
    "config = Config(\"BOF\", \"V\", epochs=5, batch_size=batch_size, lr=0.001, l2_reg=0.01,\n",
    "                  l1_reg=0.01, LSTM_layers=3, LSTM_hidden=128, num_blocks=7,\n",
    "                  feat=16, encoder_layers=2, to_print=False, M_lst=M_lst, dataset=\"Small\")\n",
    "\n",
    "BoF_train(config, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, val = BOFdummyDataset(N=1000, bof=small_bof)\n",
    "model = get_model(config)\n",
    "model.load_state_dict(torch.load(\"Dummy_BoF.pt\"))\n",
    "model.eval()\n",
    "for data in val:\n",
    "    sample = data[0][0]\n",
    "    plt.plot(ts, sample.detach(), label=\"real\")\n",
    "    \n",
    "    out, residual, interp = model.interpret(data)\n",
    "    \n",
    "    pred_sample = pred[0][0]\n",
    "    plt.plot(ts, pred_sample.detach(), label=\"pred\")\n",
    "    plt.plot(ts, residuals[0][0].detach(), label=\"res\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(interp[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmyFFXQVnKBd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = root_dir + \"Dataset/Big/{}.pt\"\n",
    "vel_path, M_path, param_path = data_path.format('vel'), data_path.format('amounts'), data_path.format('param')\n",
    "train_big, val_big = load_torch_data(vel_path, M_path, param_path, batch_size=64, M_lst=[], train=0.8)\n",
    "\n",
    "bof = [sins]\n",
    "SM1D = BoF(num_blocks=max_M, bof=bof)\n",
    "for i in train_big:\n",
    "    v, m, p = i\n",
    "    pred = SM1D(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKdrxa1Q0Sja",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict parameters and print them\n",
    "data_path = root_dir + \"Dataset/Big/{}.pt\"\n",
    "vel_path, M_path, param_path = data_path.format('vel'), data_path.format('amounts'), data_path.format('param')\n",
    "train_big, val_big = load_torch_data(vel_path, M_path, param_path, batch_size=64, M_lst=[2,3,4], train=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SM1D = SubMovLSTM(hidden_size=64, num_layers=3)\n",
    "SM1D.load_state_dict(torch.load('LSTM bi_M234_64_3_p+.pt'))\n",
    "SM1D.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, (batch_vel, batch_M, batch_param) in enumerate(val_big):\n",
    "        batch_vel = batch_vel.float()\n",
    "        pred_param = SM1D(batch_vel)\n",
    "        pred_M = calculate_M(pred_param)\n",
    "        correct += torch.sum(pred_M == batch_M)\n",
    "        total += len(batch_M)\n",
    "        \n",
    "        # # clear all infeasible values from M\n",
    "        # P = pred_param.view(pred_param.shape[0],max_M,dim+2)\n",
    "        # time_mask = (P[:, :, 0] < 0)\n",
    "        # duration_mask = (P[:, :, 1] < duration_min)\n",
    "        # amp_mask = (torch.abs(P[:, :, 2:]) < 0.05).any(dim=2)\n",
    "        # mask = (time_mask | duration_mask | amp_mask).unsqueeze(-1)\n",
    "        # pred_param = P * (~mask)\n",
    "                \n",
    "#         for s in range(10):\n",
    "#             plt.plot(ts, batch_vel[s][0].detach(), 'red', label=f'true M: {batch_M[s]}', linestyle=':', linewidth=1)\n",
    "#             plt.plot(ts, pred_vel[s][0].detach(), 'black', label=f'pred : {pred_M[s]}', linestyle=':', linewidth=1)\n",
    "\n",
    "            \n",
    "#             plt.legend()\n",
    "#             plt.ylim([-3,3])\n",
    "#             plt.title(\"Red = True sm\")\n",
    "#             plt.show()\n",
    "#         break\n",
    "    \n",
    "    print(\"Total accuracy: \", correct / total * 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Conclusions from My-Model section:\n",
    "### Models:\n",
    "\n",
    "1. The LSTM has the best performances. \n",
    "2. Best perfomance hyper-parameters: bi-LSTM with 5 layers of 64 hidden-layer size, Doupout=0.2, with BacthNorm1d. L2 regularization with lambda=0.0001, batch size of 64 has converged in less than 15 epochs.\n",
    "3. Bigger size of LSTMs were not able to learn properly, and converged to a trivial solution. **This is shoud be investigated**, but a plausible direction is the vanishing gradients phenomena.\n",
    "4. I didn't manage to train a BoF for Min-Jerk decomposition - Neither with CNN nor LSTM as the encoder. However, it does perform a fine decomposition for the dummy data set, so the problem is not with the architecture implementation.\n",
    "\n",
    "### TODO:\n",
    "1. Understand the differences in performance \"P\" vs. \"V\" losses **/ Undone**\n",
    "2. Split the data to highet success and lowest success and analyze it - Why? **/ Undone**\n",
    "3. Achieve high accuracy of predicted amount of sub movements ( > 0.95) **/ Done, look at tsai's section**\n",
    "4. Insert the process of chopping infeasible parameters (calculation of M) into the model with shifted ReLUs **/ Done, no changes observed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **tsai's Models section**\n",
    "\n",
    "\"tsai\" is a library that based on Pytorch and provides implementation of some state of the art models for time series.\n",
    "\n",
    "GitHub:\n",
    "https://github.com/timeseriesAI/tsai\n",
    "\n",
    "Docs:\n",
    "https://timeseriesai.github.io/tsai/\n",
    "\n",
    "It provides also a nice, easy to use, pipelines for training (as I used here in the *classification* task), but one can use tsai's models just as a regular pytorch models (as I used here for the *regression* task, since I wanted a custome loss function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## create data loaders\n",
    "\n",
    "The library supplies wrappers for the pytorch dataloaders, and are more easy-to-use.\n",
    "\n",
    "**you must run this before the bext cells**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Transform' is a layer of batch-operation to perform on the entire model's input\n",
    "class FilterTsfm(Transform):\n",
    "    def encodes(self, data):\n",
    "        return filter_signal(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data set as tsai's objects\n",
    "data_path = root_dir + \"Dataset/Big/{}.pt\"\n",
    "vel_path, M_path, param_path = data_path.format('vel'), data_path.format('amounts'), data_path.format('param')\n",
    "\n",
    "velocities = torch.load(vel_path)\n",
    "if len(velocities.shape) == 2: velocities = velocities[:, None, :]\n",
    "amounts = torch.load(M_path)\n",
    "parameters = torch.load(param_path).nan_to_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x50 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split to train set and test set\n",
    "splits = get_splits(amounts, shuffle=True, valid_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## InceptionTime for Regression Task\n",
    "This portion of code tries to train a tsai's InceptionTime model to predict the parameters matrix of the movements.\n",
    "\n",
    "**Results:**\n",
    "I've managed to receive an actual learning process only with the loss on the parameters, and not on the velocity reconstructed from.\n",
    "And yet, the results are not good enough - You can run the 'show results' cell for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Pipeline: ToFloat\n"
     ]
    }
   ],
   "source": [
    "# Create a TSDataset\n",
    "tfms  = [None, [TSRegression()]]\n",
    "dsets = TSDatasets(velocities, parameters, tfms=tfms, splits=splits, inplace=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DadaLoaders\n",
    "\n",
    "# Defining pipeline of filtering for the data - NOT WORKING for some reason...\n",
    "# batch_tfms = [FilterTsfm()]\n",
    "\n",
    "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random bunch of samples\n",
    "dls.show_batch(sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model, criterion and optimizer\n",
    "model = InceptionTime(dls.vars, max_M * (2 + dim)).to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = get_loss_function(\"P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining clip-layer to cut infeasible values from output parameters matrix\n",
    "clip = ClipOutput(duration_min, amp_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_loop(q=50):\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss = 0.\n",
    "        running_loss = 0.\n",
    "        model.train()\n",
    "\n",
    "        # Train\n",
    "        for batch_idx, batch in enumerate(dls.train):\n",
    "            inputs, targets = batch\n",
    "            N = len(inputs)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(inputs)\n",
    "\n",
    "            # calculate M prediction accuracy\n",
    "            real_counts = torch.count_nonzero(targets.view(N, -1, dim+2), dim=1)\n",
    "            real_M = real_counts[:,0]\n",
    "            pred_counts = torch.count_nonzero(clip(predictions).view(N, -1, dim+2), dim=1)\n",
    "            pred_M = pred_counts[:,0]\n",
    "            M_correctness = pred_M == real_M\n",
    "            M_accuracy = torch.sum(M_correctness) / N\n",
    "\n",
    "            # loss args: (vel, param, M, pred_vel, pred_param)\n",
    "            loss = criterion(inputs, targets, real_M, batch_reconstruct_prof(predictions), predictions)\n",
    "            epoch_train_loss += loss.item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            if batch_idx % q == q-1:\n",
    "                print(f'[{epoch + 1}, {batch_idx + 1:5d}] loss: {running_loss / q:.6f}. Accuracy: {M_accuracy}')\n",
    "                running_loss = 0.\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            avg_loss = 0.\n",
    "            plot = True\n",
    "\n",
    "            for batch in dls.valid:\n",
    "                inputs, targets = batch\n",
    "                N = len(inputs)\n",
    "                predictions = model(inputs)\n",
    "\n",
    "                real_counts = torch.count_nonzero(targets.view(N, -1, dim+2), dim=1)\n",
    "                real_M = real_counts[:,0]\n",
    "                pred_counts = torch.count_nonzero(clip(predictions).view(N, -1, dim+2), dim=1)\n",
    "                pred_M = pred_counts[:,0]\n",
    "                M_correctness = pred_M == real_M\n",
    "                M_accuracy = torch.sum(M_correctness) / N\n",
    "\n",
    "                loss = criterion(inputs, targets, real_M, batch_reconstruct_prof(predictions), predictions)\n",
    "                avg_loss += loss.item()\n",
    "                \n",
    "                if plot:\n",
    "                    # convert to minimum jerk curves\n",
    "                    param_mat = predictions.view(N, -1, dim + 2)  # shape: (N, 7, 3) in case of 1-d\n",
    "                    T = param_mat[:, :, 0]\n",
    "                    D = param_mat[:, :, 1]\n",
    "                    A = param_mat[:, :, 2:]\n",
    "                    min_jerk_profiles = minimum_jerk_velocity(T.unsqueeze(-1), D.unsqueeze(-1), A, ts, True)  # shape: (N, 7, 1, 400) in case of 1-d           \n",
    "                                        \n",
    "                    fig, axs = plt.subplots(3,3)\n",
    "                    for i in range(9):\n",
    "                        ax = axs[i%3][i//3]\n",
    "                        ax.plot(ts.cpu().detach().numpy(), inputs[i][0].cpu().detach().numpy(), 'red')\n",
    "                        param, M = predictions[i].view(max_M, 2+dim), pred_M[i]\n",
    "                        for min_j in min_jerk_profiles[i]:\n",
    "                            ax.plot(ts.cpu().detach().numpy(), min_j[0].cpu().detach().numpy(), 'black', linestyle=':', linewidth=0.7)\n",
    "                        ax.set_yticks([])\n",
    "                        ax.set_xticks([])\n",
    "                    fig.suptitle(\"Red = Real\")\n",
    "                    plt.show()\n",
    "                    plot = False\n",
    "                    \n",
    "        print(\"****************** epoch done********* . loss: \", avg_loss / len(dls.valid), \"M acc: \", M_accuracy.item() / len(dls.valid))\n",
    "        epoch_train_loss = 0.\n",
    "        torch.save(model.state_dict(), 'IncepTimeRegressor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot random reconstructed predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs, targets = dls.valid.one_batch()\n",
    "    N = len(inputs)\n",
    "    predictions = clip(model(inputs))\n",
    "    pred_vel = batch_reconstruct_prof(predictions)\n",
    "\n",
    "    real_counts = torch.count_nonzero(targets.view(N, -1, dim+2), dim=1)\n",
    "    real_M = real_counts[:,0]\n",
    "    pred_counts = torch.count_nonzero(predictions.view(N, -1, dim+2), dim=1)\n",
    "    pred_M = pred_counts[:,0]\n",
    "    M_correctness = pred_M == real_M\n",
    "    M_accuracy = torch.sum(M_correctness) / N\n",
    "\n",
    "    for i in torch.randint(low=0, high=N, size=(16,)):\n",
    "        plt.plot(ts.cpu().detach().numpy(), inputs[i][0].cpu().detach().numpy(), 'red')\n",
    "        plt.plot(ts.cpu().detach().numpy(), pred_vel[i][0].cpu().detach().numpy(), 'black', linestyle=':', linewidth=0.7)\n",
    "        plt.yticks([])\n",
    "        plt.xticks([])\n",
    "        fig.suptitle(\"Red = Real\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training with more epochs, this time include punishments in the loss\n",
    "model = InceptionTime(dls.vars, max_M * (2 + dim)).to('cuda')\n",
    "model.load_state_dict(torch.load('IncepTimeRegressor.pt'))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = get_loss_function(\"P+\")\n",
    "\n",
    "train_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## InceptionTime for Classification Task\n",
    "This portion of code tries to train a tsai's InceptionTime model to predict only the **amount** of sub movements in the sequence.\n",
    "\n",
    "**Results:**\n",
    "The model `models/M_Predictor/No_Filter_0.994_on_valid` was trained with no filtering on the inputs (for some reason it led to an error), and achieved total accuracy of **0.994** on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### You first need to run those cells in order to run any other cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Pipeline: TSClassification -- {'vocab': None, 'sort': True}\n"
     ]
    }
   ],
   "source": [
    "# Create a TSDataset\n",
    "tfms  = [None, [TSClassification()]]\n",
    "dsets = TSDatasets(velocities, amounts, tfms=tfms, splits=splits, inplace=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DadaLoaders\n",
    "\n",
    "# Defining pipeline of filtering for the data - NOT WORKING for some reason...\n",
    "# batch_tfms = [FilterTsfm()]\n",
    "\n",
    "dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "model = InceptionTime(dls.vars, dls.c)\n",
    "learn = Learner(dls, model, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### From here you can choose what cells to run as you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding best lr\n",
    "learn.lr_find()\n",
    "# found lr = 0.0008317637839354575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for 10 epochs\n",
    "callback = SaveModel(monitor='accuracy', fname='BestModel', verbose=True)\n",
    "learn.fit_one_cycle(10, lr_max=1e-5, cbs=callback)\n",
    "learn.save('10_epochs')\n",
    "learn.plot_metrics()\n",
    "learn.save_all(path='export', dls_fname='dls', model_fname='M_predictor', learner_fname='inceptTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  tensor(0.9945)\n"
     ]
    }
   ],
   "source": [
    "# loading the pre-trained model and check accuracy on the validation set\n",
    "# Note that for every runtime there is a different valid-set, hence accuracy may vary a little bit\n",
    "model = InceptionTime(dls.vars, dls.c)\n",
    "learn = Learner(dls, model, metrics=accuracy)\n",
    "learn.load('M_Predictor/No_Filter_0.994_on_valid')\n",
    "valid_probas, valid_targets, valid_preds = learn.get_preds(dl=dls.valid, with_decoded=True)\n",
    "print(\"Accuracy: \", (valid_targets == valid_preds).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ? predictions == peaks ?\n",
    "This code validate that the model can predict the amount ot movements contains different number of sub movements & peaks/troughs, and compare it to the success with the other samples.\n",
    "\n",
    "**Conclusion:**\n",
    "The model predicts in an equal way for both scenerios, pointing on the potential of the model finding a non-trivial decomposition of sub movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "peaks_n_troughs = []\n",
    "for batch in dls.valid:\n",
    "    vel = batch[0]\n",
    "    for v in vel:\n",
    "        peaks, _ = find_peaks(v[0].cpu(), height=0)\n",
    "        troughs, _ = find_peaks(-v[0].cpu(), height=0)\n",
    "        peaks_n_troughs.append(len(peaks) + len(troughs))\n",
    "\n",
    "peaks_n_troughs = torch.Tensor(peaks_n_troughs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentages of samples where #peaks != M:  3.6358333333333333\n",
      "Accuracy on those samples:  tensor(0.9574)\n"
     ]
    }
   ],
   "source": [
    "eq = torch.where(peaks_n_troughs == valid_targets)[0]\n",
    "print(\"Percentages of samples where #peaks != M: \", eq.shape[0] / valid_targets.shape[0] * 100)\n",
    "accuracy_on_eq = (valid_targets[eq] == valid_preds[eq]).float().mean()\n",
    "print(\"Accuracy on those samples: \", accuracy_on_eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on all other samples:  tensor(0.9959)\n"
     ]
    }
   ],
   "source": [
    "not_eq = torch.where(peaks_n_troughs != valid_targets)[0]\n",
    "accuracy_on_not_eq = (valid_targets[not_eq] == valid_preds[not_eq]).float().mean()\n",
    "print(\"Accuracy on all other samples: \", accuracy_on_not_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusions from tsai's Model section:\n",
    "\n",
    "1. Only `InceptionTime` was investigated here - I had no time to use different models...\n",
    "2. The classification task accuracy achieved the goal: > 0.94. The model can be found at `models/M_Predictor/No_Filter_0.994_on_valid`.\n",
    "3. I have validated the model with both validation set of the Big dataset, and the wntire Small dataset (which is unseen to the model). The results:\n",
    "* **validation set:** Total success of ~99.4%. success of ~99.5% on the data where amount of peaks & troughs is equal to M, and ~95.5% on the other data.\n",
    "* **Small dataset:** 99.43% of success, and it worth to note that this dataset have even **more** samples where the amount of peaks & troughs differ from M! (~12%). \n",
    "4. The regression task was more successful thatn My-Model section, but yet not enough. Further more, also here the loss function based on the velocity reconstruction was very week. You can see performances in the *show results* sub-section.\n",
    "\n",
    "### TODO:\n",
    "1. (1) & (2) from previous section's conclusions.\n",
    "2. Create another data set, where the overlapping of the SMs is higher, and therefor the amount of peaks & troughs is more likle to differ from actual amount of sub movements. See conclusion number (3).\n",
    "3. Get predictions of real-life, labeld data, and evaluate performance. Is the error rate of the model reflects profiles that their peaks-counting is different? (If Yes - this is a problem). What about trimmed-data from both sides?\n",
    "4. Train different models. Have a look at the *New models* line in the main github page's readme file.\n",
    "5. Try to train a BoF model using one of the tsai's model regressors."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "IDIQZpa0pIyo",
    "h1WcTYFAwaoE",
    "vbSqY_RPwdBJ",
    "7KKIJy7Qrwwf",
    "Hy3ruio7qoXF",
    "jJVeeLUyapVr",
    "dsHESajar7ta",
    "SaN8VynLpgjG",
    "rPpmhQ0Hqajv",
    "i4ri1SE9leT-",
    "L9XLhGavljLw",
    "eHu2gdN7lw2q",
    "z0Mj3HXdpVzm",
    "_IGTI5AR1rDH",
    "mMRBUvhDrEak",
    "as-NxyjapyY7",
    "NKzF0kfFljT7",
    "OGgOjYZMqdri",
    "1l82dR-MsU6s",
    "heVcSqVaqHvr",
    "4u6Ig52bk56b",
    "qYDl5U4x0YIc",
    "4aEg3YbQx71K"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m110"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
